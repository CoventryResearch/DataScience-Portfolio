{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split # to divide train and test set\n",
    "from sklearn import preprocessing # for feature scaling\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# import linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model evaluation\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Capgemini/Dropbox/Portfolio/DataScience-Portfolio/KDD-1998\n"
     ]
    }
   ],
   "source": [
    "cd Dropbox/Portfolio/DataScience-Portfolio/KDD-1998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and separate in Classifier and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "kdd = pd.read_csv('kdd_reg_fe_small.csv')\n",
    "\n",
    "\n",
    "# generate X and Y for preditions\n",
    "Y = np.ravel(kdd['0'])  # to flatten array\n",
    "X = kdd.drop('0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4829, 1244)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature scaling - normalisation\n",
    "def standarisation(train, test):\n",
    "    scaler = preprocessing.StandardScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test\n",
    "\n",
    "# Feature scaling - MinMax Scaler (scales between 0 and 1)\n",
    "def minMax_standarisation(train, test):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature selection function\n",
    "def feat_select(model, xtrain, test, ytrain):\n",
    "    selector = model\n",
    "    selector.fit(xtrain, ytrain)\n",
    "    X_train_new = selector.transform(xtrain)\n",
    "    X_test_new = selector.transform(test)\n",
    "    return X_train_new, X_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "def linReg_mod(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    regr = LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    # Measures for training set\n",
    "    print(\"Train set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(Xtrain) - Ytrain) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Rsquared: %.2f' % regr.score(Xtrain, Ytrain))\n",
    "    print('=================')\n",
    "    # Measures for testing set\n",
    "    print(\"Test set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(Xtest) - Ytest) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Rsquared: %.2f' % regr.score(Xtest, Ytest))\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print results of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print results of model\n",
    "def print_results(clf, X_train, Y_train, X_test, Y_test):\n",
    "    print(\"Train set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((clf.predict(X_train) - Y_train) ** 2))\n",
    "    print('Rsquared: %.2f' % clf.score(X_train, Y_train))\n",
    "    print('=================')\n",
    "    print(\"Test set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((clf.predict(X_test) - Y_test) ** 2))\n",
    "    print('Rsquared: %.2f' % clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split train and test set and normalise predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split test and train and normalise\n",
    "def split_standarise(X,Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "    X_train, X_test = standarisation(X_train, X_test)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2897, 1244), (1932, 1244))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate testing and training set + normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2897, 125), (1932, 125))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select top 10% features\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 10), X_train, X_test, Y_train)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 51.86\n",
      "Rsquared: 0.67\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 92.95\n",
      "Rsquared: 0.38\n"
     ]
    }
   ],
   "source": [
    "# run first linear model with 287 features\n",
    "regr = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting to the training set. We see however, that with these new set of engineered variables, the overfitting is less than in the previous notebook, with features asis\n",
    "\n",
    "### Second linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test and normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "# select 10 best features\n",
    "X_train, X_test = feat_select(SelectKBest(f_regression, k=10), X_train, X_test, Y_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 68.21\n",
      "Rsquared: 0.57\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 83.65\n",
      "Rsquared: 0.44\n"
     ]
    }
   ],
   "source": [
    "# run second linear model with 10 features\n",
    "regr2 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better model. Although the R squared is quite low.\n",
    "\n",
    "### Third linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sep train and test, normalise, select top 10% features\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 10), X_train, X_test, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursive Feature Selection (RFS)\n",
    "regr_RFS = SelectFromModel(regr, prefit=True)\n",
    "X_train = regr_RFS.transform(X_train)\n",
    "X_test = regr_RFS.transform(X_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 96.06\n",
      "Rsquared: 0.39\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 101.07\n",
      "Rsquared: 0.33\n"
     ]
    }
   ],
   "source": [
    "regr3 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfits.\n",
    "\n",
    "### Try different feature scaling (MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 68.21\n",
      "Rsquared: 0.57\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 83.65\n",
      "Rsquared: 0.44\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "\n",
    "# normalise features with MinMax\n",
    "X_train, X_test = minMax_standarisation(X_train, X_test)\n",
    "\n",
    "# select 10 best features\n",
    "X_train, X_test = feat_select(SelectKBest(f_regression, k=10), X_train, X_test, Y_train)\n",
    "regr3 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement with respect to first model with 10 best features\n",
    "\n",
    "## Try normalizing label with log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.63\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.59\n"
     ]
    }
   ],
   "source": [
    "regr3 = linReg_mod(X_train, np.log(Y_train), X_test, np.log(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is much better than its equivalent in the previous notebook, with an R squared of 0.59 here versus 0.51 previously. \n",
    "\n",
    "### Feature selection with grid search and BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=2897, n_folds=2, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('anova', SelectPercentile(percentile=10,\n",
       "         score_func=<function f_regression at 0x10fa78048>)), ('ridge', BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'anova__percentile': [2, 5, 10, 20, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "ridge = BayesianRidge()      # model object\n",
    "cv = KFold(len(Y_train), 2)  # cross-validation generator for model selection\n",
    "anova = SelectPercentile(f_regression) # feature selection\n",
    "\n",
    "clf = Pipeline([('anova', anova), ('ridge', ridge)])\n",
    "\n",
    "# Select the optimal percentage of features with grid search\n",
    "clf = GridSearchCV(clf, {'anova__percentile': [2, 5, 10, 20, 30, 50]}, cv=cv)\n",
    "clf.fit(X_train, Y_train)  # set the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anova__percentile': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 64.73\n",
      "Rsquared: 0.59\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 82.83\n",
      "Rsquared: 0.45\n"
     ]
    }
   ],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not better than the linear regression with 10 features.\n",
    "\n",
    "### Same as above but with feature agglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=2897, n_folds=2, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('ward', FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',\n",
       "           connectivity=None, linkage='ward', memory=Memory(cachedir=None),\n",
       "           n_clusters=10, n_components=None,\n",
       "           pooling_func=<function mean at 0x10bb7f378>)), ('ridge', BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'ward__n_clusters': [10, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "ridge = BayesianRidge() # model object\n",
    "\n",
    "# Ward agglomeration followed by BayesianRidge\n",
    "ward = FeatureAgglomeration(n_clusters=10)\n",
    "clf = Pipeline([('ward', ward), ('ridge', ridge)])\n",
    "# Select the optimal number of parcels with grid search\n",
    "clf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)\n",
    "clf.fit(X_train, Y_train)  # set the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ward__n_clusters': 30}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 99.27\n",
      "Rsquared: 0.37\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 109.83\n",
      "Rsquared: 0.27\n"
     ]
    }
   ],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 69.79\n",
      "Rsquared: 0.56\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 84.22\n",
      "Rsquared: 0.44\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 60.56\n",
      "Rsquared: 0.62\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 86.88\n",
      "Rsquared: 0.42\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 60.53\n",
      "Rsquared: 0.62\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 86.93\n",
      "Rsquared: 0.42\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 60.53\n",
      "Rsquared: 0.62\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 86.95\n",
      "Rsquared: 0.42\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 5), X_train, X_test, Y_train)\n",
    "\n",
    "# SVM regularization parameter\n",
    "for C in [0.01, .1, 1.0, 10]:\n",
    "    # SVC with a Linear Kernel \n",
    "    svr = SVR(kernel='linear', C=C).fit(X_train, Y_train)\n",
    "    print_results(svr, X_train, Y_train, X_test, Y_test)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The linear regression model utilising 10 best features selected by univariate feature selection following the previous feature engineering beats the models developed in the previous notebook (kdd_regression). It offers at the same time a simpler model that allows understanding of the importance of the features in the predictions of the amount donated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "selector = SelectKBest(f_regression, k=10)\n",
    "selector.fit(X_train, Y_train)\n",
    "X_train_new = selector.transform(X_train)\n",
    "X_test_new = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 10)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pval = pd.Series(selector.pvalues_)\n",
    "ind = pval.sort_values().head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAXRAMNT_log</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LASTGIFT_log</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RFA_2_L1G</td>\n",
       "      <td>1.115719e-123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MINRAMNT_log</td>\n",
       "      <td>1.566036e-119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RFA_2F_log</td>\n",
       "      <td>1.416886e-89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MDMAUD_XXXX_asi</td>\n",
       "      <td>2.267747e-62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RFA_6_A1G</td>\n",
       "      <td>4.419824e-56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RFA_14_A1G</td>\n",
       "      <td>8.295002e-52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RFA_6_L3C</td>\n",
       "      <td>2.333165e-50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RFA_2_L4D</td>\n",
       "      <td>3.687283e-49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature        p_value\n",
       "0     MAXRAMNT_log   0.000000e+00\n",
       "1     LASTGIFT_log   0.000000e+00\n",
       "2        RFA_2_L1G  1.115719e-123\n",
       "3     MINRAMNT_log  1.566036e-119\n",
       "4       RFA_2F_log   1.416886e-89\n",
       "5  MDMAUD_XXXX_asi   2.267747e-62\n",
       "6        RFA_6_A1G   4.419824e-56\n",
       "7       RFA_14_A1G   8.295002e-52\n",
       "8        RFA_6_L3C   2.333165e-50\n",
       "9        RFA_2_L4D   3.687283e-49"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe containing the features utilised for the predictions in the linear model\n",
    "feat = pd.DataFrame(X.columns[ind], columns = ['feature'])\n",
    "feat['p_value']= pd.DataFrame(np.array(pval.sort_values().head(10)))\n",
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features tha strongly contribute to the prediction of the donation amount are the same as in the previous notebook, which are the maximum amount donated (MAXRAMNT), the amount donated last (LASTGIFT) and in this case instead of the average donated we have the mnimum amount donated (MINRAMNT). I believe that the average and the minimum amount donated are probably hihgly correlated, and we could possibly swap this variables without affecting the predictive power of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
