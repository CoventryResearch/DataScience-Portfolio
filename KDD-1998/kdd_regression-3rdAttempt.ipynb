{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split # to divide train and test set\n",
    "from sklearn import preprocessing # for feature scaling\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# import linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model evaluation\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Capgemini/Dropbox/Portfolio/DataScience-Portfolio/KDD-1998\n"
     ]
    }
   ],
   "source": [
    "cd Dropbox/Portfolio/DataScience-Portfolio/KDD-1998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and separate in Classifier and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "kdd = pd.read_csv('kdd_reg_fe_log_small.csv')\n",
    "\n",
    "\n",
    "# generate X and Y for preditions\n",
    "Y = np.ravel(kdd['0'])  # to flatten array\n",
    "X = kdd.drop('0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4829, 1244)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature scaling - normalisation\n",
    "def standarisation(train, test):\n",
    "    scaler = preprocessing.StandardScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test\n",
    "\n",
    "# Feature scaling - MinMax Scaler (scales between 0 and 1)\n",
    "def minMax_standarisation(train, test):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature selection function\n",
    "def feat_select(model, xtrain, test, ytrain):\n",
    "    selector = model\n",
    "    selector.fit(xtrain, ytrain)\n",
    "    X_train_new = selector.transform(xtrain)\n",
    "    X_test_new = selector.transform(test)\n",
    "    return X_train_new, X_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "def linReg_mod(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    regr = LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    # Measures for training set\n",
    "    print(\"Train set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(Xtrain) - Ytrain) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Rsquared: %.2f' % regr.score(Xtrain, Ytrain))\n",
    "    print('=================')\n",
    "    # Measures for testing set\n",
    "    print(\"Test set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(Xtest) - Ytest) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Rsquared: %.2f' % regr.score(Xtest, Ytest))\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print results of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print results of model\n",
    "def print_results(clf, X_train, Y_train, X_test, Y_test):\n",
    "    print(\"Train set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((clf.predict(X_train) - Y_train) ** 2))\n",
    "    print('Rsquared: %.2f' % clf.score(X_train, Y_train))\n",
    "    print('=================')\n",
    "    print(\"Test set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((clf.predict(X_test) - Y_test) ** 2))\n",
    "    print('Rsquared: %.2f' % clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split train and test set and normalise predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split test and train and normalise\n",
    "def split_standarise(X,Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "    X_train, X_test = standarisation(X_train, X_test)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2897, 1244), (1932, 1244))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate testing and training set + normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2897, 125), (1932, 125))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select top 10% features\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 10), X_train, X_test, Y_train)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.14\n",
      "Rsquared: 0.66\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.17\n",
      "Rsquared: 0.58\n"
     ]
    }
   ],
   "source": [
    "# run first linear model with 287 features\n",
    "regr = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a substantial less overfitting when we use 125 of these features, compared with the same model in the previous notebooks.\n",
    "\n",
    "### Second linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test and normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "# select 10 best features\n",
    "X_train, X_test = feat_select(SelectKBest(f_regression, k=10), X_train, X_test, Y_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.63\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.59\n"
     ]
    }
   ],
   "source": [
    "# run second linear model with 10 features\n",
    "regr2 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better model. Performance is similar to the one built on the previous notebook, used to estimate the log(donation amount)\n",
    "\n",
    "### Third linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sep train and test, normalise, select top 10% features\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 10), X_train, X_test, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursive Feature Selection (RFS)\n",
    "regr_RFS = SelectFromModel(regr, prefit=True)\n",
    "X_train = regr_RFS.transform(X_train)\n",
    "X_test = regr_RFS.transform(X_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.19\n",
      "Rsquared: 0.56\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.19\n",
      "Rsquared: 0.52\n"
     ]
    }
   ],
   "source": [
    "regr3 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good performance as the previous one. \n",
    "\n",
    "### Try different feature scaling (MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.63\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.59\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "\n",
    "# normalise features with MinMax\n",
    "X_train, X_test = minMax_standarisation(X_train, X_test)\n",
    "\n",
    "# select 10 best features\n",
    "X_train, X_test = feat_select(SelectKBest(f_regression, k=10), X_train, X_test, Y_train)\n",
    "regr3 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar performance.\n",
    "\n",
    "### Feature selection with grid search and BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=2897, n_folds=2, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('anova', SelectPercentile(percentile=10,\n",
       "         score_func=<function f_regression at 0x10fa7d048>)), ('ridge', BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'anova__percentile': [2, 5, 10, 20, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "ridge = BayesianRidge()      # model object\n",
    "cv = KFold(len(Y_train), 2)  # cross-validation generator for model selection\n",
    "anova = SelectPercentile(f_regression) # feature selection\n",
    "\n",
    "clf = Pipeline([('anova', anova), ('ridge', ridge)])\n",
    "\n",
    "# Select the optimal percentage of features with grid search\n",
    "clf = GridSearchCV(clf, {'anova__percentile': [2, 5, 10, 20, 30, 50]}, cv=cv)\n",
    "clf.fit(X_train, Y_train)  # set the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anova__percentile': 2}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.15\n",
      "Rsquared: 0.64\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.60\n"
     ]
    }
   ],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bayesian Ridge is slightly better than the linear regression.\n",
    "\n",
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.15\n",
      "Rsquared: 0.64\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.58\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 0.15\n",
      "Rsquared: 0.64\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.58\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 0.15\n",
      "Rsquared: 0.64\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.58\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 0.15\n",
      "Rsquared: 0.64\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.16\n",
      "Rsquared: 0.58\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 5), X_train, X_test, Y_train)\n",
    "\n",
    "# SVM regularization parameter\n",
    "for C in [0.01, .1, 1.0, 10]:\n",
    "    # SVC with a Linear Kernel \n",
    "    svr = SVR(kernel='linear', C=C).fit(X_train, Y_train)\n",
    "    print_results(svr, X_train, Y_train, X_test, Y_test)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Bayesian Ridge seems to be the best model. It would need some model optimisation, so I will leave it for later and will go forward with the linear regression with 10 best features for the rest of the excercise.\n",
    "\n",
    "The best model is the linear regression with the selected 10 best features, as it renders the lower sum of square errors and higher R squared in both training and test set. Here I will re-do the model to identify the 10 best features and their coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "anova = SelectPercentile(f_regression, 2) # feature selection\n",
    "anova.fit(X_train, Y_train)\n",
    "X_train_new = anova.transform(X_train)\n",
    "X_test_new = anova.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 25)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pval = pd.Series(anova.pvalues_)\n",
    "pval.sort_values().head(10)\n",
    "ind = pval.sort_values().head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MAXRAMNT_log</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LASTGIFT_log</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RFA_2F_asi</td>\n",
       "      <td>1.392417e-197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MINRAMNT_log</td>\n",
       "      <td>1.048832e-189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RFA_2_L4D</td>\n",
       "      <td>1.028061e-153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RFA_2_L1G</td>\n",
       "      <td>2.348069e-90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RFA_3_S4D</td>\n",
       "      <td>1.651415e-89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RFA_6_S4D</td>\n",
       "      <td>7.223653e-65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RFA_2A_F</td>\n",
       "      <td>5.346937e-56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PEPSTRFL</td>\n",
       "      <td>1.575762e-55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature        p_value\n",
       "0  MAXRAMNT_log   0.000000e+00\n",
       "1  LASTGIFT_log   0.000000e+00\n",
       "2    RFA_2F_asi  1.392417e-197\n",
       "3  MINRAMNT_log  1.048832e-189\n",
       "4     RFA_2_L4D  1.028061e-153\n",
       "5     RFA_2_L1G   2.348069e-90\n",
       "6     RFA_3_S4D   1.651415e-89\n",
       "7     RFA_6_S4D   7.223653e-65\n",
       "8      RFA_2A_F   5.346937e-56\n",
       "9      PEPSTRFL   1.575762e-55"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = pd.DataFrame(X.columns[ind], columns = ['feature'])\n",
    "feat['p_value']= pd.DataFrame(np.array(pval.sort_values().head(10)))\n",
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same values as in kdd_regression_2nd_Attempt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
