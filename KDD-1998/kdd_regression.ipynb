{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split # to divide train and test set\n",
    "from sklearn import preprocessing # for feature scaling\n",
    "\n",
    "# feature selection\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# import linear model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# model evaluation\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Capgemini/Dropbox/Portfolio/DataScience-Portfolio/KDD-1998\n"
     ]
    }
   ],
   "source": [
    "cd Dropbox/Portfolio/DataScience-Portfolio/KDD-1998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and separate in Classifier and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "kdd = pd.read_csv('data_reg2.csv')\n",
    "\n",
    "# generate X and Y for preditions\n",
    "Y = np.ravel(kdd.TARGET_D)  # to flatten array\n",
    "X = kdd.drop('TARGET_D', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4829, 1913)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature scaling - normalisation\n",
    "def standarisation(train, test):\n",
    "    scaler = preprocessing.StandardScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test\n",
    "\n",
    "# Feature scaling - MinMax Scaler (scales between 0 and 1)\n",
    "def minMax_standarisation(train, test):\n",
    "    scaler = preprocessing.MinMaxScaler().fit(train)\n",
    "    train = scaler.transform(train)\n",
    "    test = scaler.transform(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature selection function\n",
    "def feat_select(model, xtrain, test, ytrain):\n",
    "    selector = model\n",
    "    selector.fit(xtrain, ytrain)\n",
    "    X_train_new = selector.transform(xtrain)\n",
    "    X_test_new = selector.transform(test)\n",
    "    return X_train_new, X_test_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "def linReg_mod(Xtrain, Ytrain, Xtest, Ytest):\n",
    "    regr = LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(Xtrain, Ytrain)\n",
    "    \n",
    "    # Measures for training set\n",
    "    print(\"Train set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(Xtrain) - Ytrain) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Rsquared: %.2f' % regr.score(Xtrain, Ytrain))\n",
    "    print('=================')\n",
    "    # Measures for testing set\n",
    "    print(\"Test set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((regr.predict(Xtest) - Ytest) ** 2))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Rsquared: %.2f' % regr.score(Xtest, Ytest))\n",
    "    return regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to print results of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print results of model\n",
    "def print_results(clf, X_train, Y_train, X_test, Y_test):\n",
    "    print(\"Train set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((clf.predict(X_train) - Y_train) ** 2))\n",
    "    print('Rsquared: %.2f' % clf.score(X_train, Y_train))\n",
    "    print('=================')\n",
    "    print(\"Test set\")\n",
    "    print(\"Residual sum of squares: %.2f\" % np.mean((clf.predict(X_test) - Y_test) ** 2))\n",
    "    print('Rsquared: %.2f' % clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to split train and test set and normalise predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to split test and train and normalise\n",
    "def split_standarise(X,Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "    X_train, X_test = standarisation(X_train, X_test)\n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2897, 1913), (1932, 1913))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate testing and training set + normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2897, 192), (1932, 192))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select top 10% features\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 10), X_train, X_test, Y_train)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 51.26\n",
      "Rsquared: 0.68\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 1048359703987778702278656.00\n",
      "Rsquared: -6988277458269901946880.00\n"
     ]
    }
   ],
   "source": [
    "# run first linear model with 192 features\n",
    "regr = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting to the training set.\n",
    "\n",
    "### Second linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test and normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "# select 10 best features\n",
    "X_train, X_test = feat_select(SelectKBest(f_regression, k=10), X_train, X_test, Y_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 73.53\n",
      "Rsquared: 0.54\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 84.51\n",
      "Rsquared: 0.44\n"
     ]
    }
   ],
   "source": [
    "# run second linear model with 10 features\n",
    "regr2 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better model. Although the R squared is quite low.\n",
    "\n",
    "### Third linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sep train and test, normalise, select top 10% features\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 10), X_train, X_test, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recursive Feature Selection (RFS)\n",
    "regr_RFS = SelectFromModel(regr, prefit=True)\n",
    "X_train = regr_RFS.transform(X_train)\n",
    "X_test = regr_RFS.transform(X_test)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 91.55\n",
      "Rsquared: 0.42\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 4944838232850513219106635776.00\n",
      "Rsquared: -32961875037714399994314752.00\n"
     ]
    }
   ],
   "source": [
    "regr3 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfits.\n",
    "\n",
    "### Try different feature scaling (MinMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 73.53\n",
      "Rsquared: 0.54\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 84.51\n",
      "Rsquared: 0.44\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=42)\n",
    "\n",
    "# normalise features with MinMax\n",
    "X_train, X_test = minMax_standarisation(X_train, X_test)\n",
    "\n",
    "# select 10 best features\n",
    "X_train, X_test = feat_select(SelectKBest(f_regression, k=10), X_train, X_test, Y_train)\n",
    "regr3 = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement with respect to first model with 10 best features\n",
    "\n",
    "## Try normalizing label with log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.19\n",
      "Rsquared: 0.56\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.19\n",
      "Rsquared: 0.51\n"
     ]
    }
   ],
   "source": [
    "regr3 = linReg_mod(X_train, np.log(Y_train), X_test, np.log(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better is obtained if we estimate the log of the amount donated.\n",
    "\n",
    "### Feature selection with grid search and BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=2897, n_folds=2, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('anova', SelectPercentile(percentile=10,\n",
       "         score_func=<function f_regression at 0x10fb89048>)), ('ridge', BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'anova__percentile': [2, 5, 10, 20, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# split and normalise\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "ridge = BayesianRidge()      # model object\n",
    "cv = KFold(len(Y_train), 2)  # cross-validation generator for model selection\n",
    "anova = SelectPercentile(f_regression) # feature selection\n",
    "\n",
    "clf = Pipeline([('anova', anova), ('ridge', ridge)])\n",
    "\n",
    "# Select the optimal percentage of features with grid search\n",
    "clf = GridSearchCV(clf, {'anova__percentile': [2, 5, 10, 20, 30, 50]}, cv=cv)\n",
    "clf.fit(X_train, Y_train)  # set the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anova__percentile': 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 65.28\n",
      "Rsquared: 0.59\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 78.39\n",
      "Rsquared: 0.48\n"
     ]
    }
   ],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a little bit better than the linear regression with 10 features.\n",
    "\n",
    "### Same as above but with feature agglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.KFold(n=2897, n_folds=2, shuffle=False, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=Pipeline(steps=[('ward', FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',\n",
       "           connectivity=None, linkage='ward', memory=Memory(cachedir=None),\n",
       "           n_clusters=10, n_components=None,\n",
       "           pooling_func=<function mean at 0x10bcad378>)), ('ridge', BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "       fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "       normalize=False, tol=0.001, verbose=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'ward__n_clusters': [10, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import FeatureAgglomeration\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "ridge = BayesianRidge() # model object\n",
    "\n",
    "# Ward agglomeration followed by BayesianRidge\n",
    "ward = FeatureAgglomeration(n_clusters=10)\n",
    "clf = Pipeline([('ward', ward), ('ridge', ridge)])\n",
    "# Select the optimal number of parcels with grid search\n",
    "clf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)\n",
    "clf.fit(X_train, Y_train)  # set the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ward__n_clusters': 30}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 102.37\n",
      "Rsquared: 0.35\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 102.09\n",
      "Rsquared: 0.32\n"
     ]
    }
   ],
   "source": [
    "print_results(clf, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat worse model than the previous ones\n",
    "\n",
    "### Support vector regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 76.66\n",
      "Rsquared: 0.52\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 79.74\n",
      "Rsquared: 0.47\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 83.36\n",
      "Rsquared: 0.47\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 75.70\n",
      "Rsquared: 0.50\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 92.47\n",
      "Rsquared: 0.42\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 79.10\n",
      "Rsquared: 0.47\n",
      "\n",
      "\n",
      "Train set\n",
      "Residual sum of squares: 94.01\n",
      "Rsquared: 0.41\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 79.27\n",
      "Rsquared: 0.47\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 5), X_train, X_test, Y_train)\n",
    "\n",
    "# SVM regularization parameter\n",
    "for C in [0.01, .1, 1.0, 10]:\n",
    "    # SVC with a Linear Kernel \n",
    "    svr = SVR(kernel='linear', C=C).fit(X_train, Y_train)\n",
    "    print_results(svr, X_train, Y_train, X_test, Y_test)\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector regression with a C parameter of 0.1 offers the best model at the minute, as it shows the highest R squared for the training set (0.5)\n",
    "\n",
    "### Try same model with the logarithm of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 0.19\n",
      "Rsquared: 0.56\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 0.18\n",
      "Rsquared: 0.55\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel='linear', C=0.1).fit(X_train, (np.log(Y_train)))\n",
    "print_results(svr, X_train, np.log(Y_train), X_test, np.log(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model offers a 10% improvement with respect to the previous one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1\n",
      "Train set\n",
      "Residual sum of squares: 91.74\n",
      "Rsquared: 0.42\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 93.25\n",
      "Rsquared: 0.38\n",
      "\n",
      "\n",
      "0.1 2\n",
      "Train set\n",
      "Residual sum of squares: 90.39\n",
      "Rsquared: 0.43\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 105.47\n",
      "Rsquared: 0.30\n",
      "\n",
      "\n",
      "0.1 3\n",
      "Train set\n",
      "Residual sum of squares: 79.96\n",
      "Rsquared: 0.49\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 106.63\n",
      "Rsquared: 0.29\n",
      "\n",
      "\n",
      "0.1 4\n",
      "Train set\n",
      "Residual sum of squares: 72.12\n",
      "Rsquared: 0.54\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 119.36\n",
      "Rsquared: 0.20\n",
      "\n",
      "\n",
      "1.0 1\n",
      "Train set\n",
      "Residual sum of squares: 76.62\n",
      "Rsquared: 0.52\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 79.72\n",
      "Rsquared: 0.47\n",
      "\n",
      "\n",
      "1.0 2\n",
      "Train set\n",
      "Residual sum of squares: 66.59\n",
      "Rsquared: 0.58\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 93.24\n",
      "Rsquared: 0.38\n",
      "\n",
      "\n",
      "1.0 3\n",
      "Train set\n",
      "Residual sum of squares: 54.07\n",
      "Rsquared: 0.66\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 114.09\n",
      "Rsquared: 0.24\n",
      "\n",
      "\n",
      "1.0 4\n",
      "Train set\n",
      "Residual sum of squares: 50.46\n",
      "Rsquared: 0.68\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 155.41\n",
      "Rsquared: -0.04\n",
      "\n",
      "\n",
      "10 1\n",
      "Train set\n",
      "Residual sum of squares: 83.39\n",
      "Rsquared: 0.47\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 75.72\n",
      "Rsquared: 0.50\n",
      "\n",
      "\n",
      "10 2\n",
      "Train set\n",
      "Residual sum of squares: 47.65\n",
      "Rsquared: 0.70\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 97.15\n",
      "Rsquared: 0.35\n",
      "\n",
      "\n",
      "10 3\n",
      "Train set\n",
      "Residual sum of squares: 39.69\n",
      "Rsquared: 0.75\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 172.10\n",
      "Rsquared: -0.15\n",
      "\n",
      "\n",
      "10 4\n",
      "Train set\n",
      "Residual sum of squares: 39.74\n",
      "Rsquared: 0.75\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 159.15\n",
      "Rsquared: -0.06\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try Support vector regression with polynomial kernel\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 5), X_train, X_test, Y_train)\n",
    "\n",
    "# SVM regularization parameter\n",
    "for C in [.1, 1.0, 10]:\n",
    "    for degree in range(1,5):\n",
    "        # SVR with a Poly Kernel\n",
    "        svr = SVR(kernel='poly', degree = degree, C=C).fit(X_train, Y_train)\n",
    "        print(C, degree)\n",
    "        print_results(svr, X_train, Y_train, X_test, Y_test)\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the polynomial kernel overfits to the training set.\n",
    "\n",
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11879738  0.08759205  0.07831258  0.07250979  0.06881569]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "X_train, X_test = feat_select(SelectPercentile(f_regression, percentile = 5), X_train, X_test, Y_train)\n",
    "\n",
    "pca = PCA(n_components=5, copy=True, whiten=False)\n",
    "pca.fit(X_train)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 5)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pca.transform(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Residual sum of squares: 92.40\n",
      "Rsquared: 0.42\n",
      "=================\n",
      "Test set\n",
      "Residual sum of squares: 95.16\n",
      "Rsquared: 0.37\n"
     ]
    }
   ],
   "source": [
    "regrPca = linReg_mod(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The best model is the support vector regression with a C of 0.1, utilising the 5% best features selected by univariate feature selection. This model estimates the log(donation amount). The R squared is 0.55.\n",
    "\n",
    "The second best model is the bayesianRidge utilising 2% best features selected by univariate feature selection. I haven't optimised the parameters of the Bayesian Ridge, and I have not attempted to estimate the log of the donation amount, which will most likely generate an improvement in the predictive power of the model, so there is definitely some room to improve this model.\n",
    "\n",
    "The third model is a linear regression model utilising the 10 best features selected by univariate feature selection. I will explore this model further below, to identify the variables that are important contributors for the accurate prediction of the donation amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = split_standarise(X,Y)\n",
    "\n",
    "selector = SelectKBest(f_regression, k=10)\n",
    "selector.fit(X_train, Y_train)\n",
    "X_train_new = selector.transform(X_train)\n",
    "X_test_new = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2897, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301      0.000000e+00\n",
       "1894    2.883861e-208\n",
       "303     2.781228e-164\n",
       "436     1.115719e-123\n",
       "300     1.237474e-114\n",
       "305      7.678342e-88\n",
       "451      4.169689e-78\n",
       "521      4.697240e-78\n",
       "1891     2.228358e-74\n",
       "1904     3.024232e-71\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pval = pd.Series(selector.pvalues_)\n",
    "pval.sort_values().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([301, 1894, 303, 436, 300, 305, 451, 521, 1891, 1904], dtype='int64')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = pval.sort_values().head(10).index\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      LASTGIFT\n",
       "1      RFA_2A_G\n",
       "2       AVGGIFT\n",
       "3     RFA_2_L1G\n",
       "4      MAXRAMNT\n",
       "5        RFA_2F\n",
       "6     RFA_3_A1G\n",
       "7     RFA_4_A1G\n",
       "8      RFA_2A_D\n",
       "9    MDMAUD_A_C\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns with the most significant p-values: these were used in the model\n",
    "pd.Series(X.columns[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LASTGIFT</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RFA_2A_G</td>\n",
       "      <td>2.883861e-208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVGGIFT</td>\n",
       "      <td>2.781228e-164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RFA_2_L1G</td>\n",
       "      <td>1.115719e-123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAXRAMNT</td>\n",
       "      <td>1.237474e-114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RFA_2F</td>\n",
       "      <td>7.678342e-88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RFA_3_A1G</td>\n",
       "      <td>4.169689e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RFA_4_A1G</td>\n",
       "      <td>4.697240e-78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RFA_2A_D</td>\n",
       "      <td>2.228358e-74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MDMAUD_A_C</td>\n",
       "      <td>3.024232e-71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature        p_value\n",
       "0    LASTGIFT   0.000000e+00\n",
       "1    RFA_2A_G  2.883861e-208\n",
       "2     AVGGIFT  2.781228e-164\n",
       "3   RFA_2_L1G  1.115719e-123\n",
       "4    MAXRAMNT  1.237474e-114\n",
       "5      RFA_2F   7.678342e-88\n",
       "6   RFA_3_A1G   4.169689e-78\n",
       "7   RFA_4_A1G   4.697240e-78\n",
       "8    RFA_2A_D   2.228358e-74\n",
       "9  MDMAUD_A_C   3.024232e-71"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = pd.DataFrame(X.columns[ind], columns = ['feature'])\n",
    "feat['p_value']= pd.DataFrame(np.array(pval.sort_values().head(10)))\n",
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The most important features for the prediction of the donation amount are the amount donated last (LASTGIFT), the average amount donated in general (AVGGIFT), and the maximum amount donated so far (MAXRAMNT) among other variables that indicate certain codes pertinent to the charity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
